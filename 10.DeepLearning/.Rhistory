library(keras)
library(tensorflow)
library(reticulate)
py_config()
img_dir <- "book_images"
image_names <- list.files(img_dir)
num_images <- length(image_names)
x <- array(dim = c(num_images, 224, 224, 3))
for(i in 1:num_images) {
img_path <- paste(img_dir, image_names[i], sep = "/")
img <- image_load(img_path, target_size = c(224, 224))
x[i,,,] <- image_to_array(img)
}
library(keras)
library(tensorflow)
library(reticulate)
use_python("C:/Users/dell/miniconda3/envs/r-reticulate/python.exe")
library(keras)
library(tensorflow)
library(reticulate)
use_python("C:/Users/dell/miniconda3/envs/r-reticulate/python.exe")
library(keras)
library(tensorflow)
library(reticulate)
use_python("C:/Users/dell/miniconda3/envs/r-reticulate/python.exe")
old_path <- Sys.getenv("PATH")
new_path <- paste(
"C:/Users/dell/miniconda3/envs/r-reticulate/Library/bin",
"C:/Users/dell/miniconda3/envs/r-reticulate/Scripts",
old_path,
sep = ";"
)
pillow
old_path <- Sys.getenv("PATH")
new_path <- paste(
"C:/Users/dell/miniconda3/envs/r-reticulate/Library/bin",
"C:/Users/dell/miniconda3/envs/r-reticulate/Scripts",
old_path,
sep = ";"
)
library(keras)
library(tensorflow)
library(reticulate)
use_python("C:/Users/dell/miniconda3/envs/r-reticulate/python.exe")
old_path <- Sys.getenv("PATH")
new_path <- paste(
"C:/Users/dell/miniconda3/envs/r-reticulate/Library/bin",
"C:/Users/dell/miniconda3/envs/r-reticulate/Scripts",
old_path,
sep = ";"
)
library(keras)
library(tensorflow)
library(reticulate)
use_python("C:/Users/dell/miniconda3/envs/r-reticulate/python.exe")
py_config()
max_feature <- 10000
imdb <- dataset_imdb(num_words = max_feature)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb
word_index <- dataset_imdb_word_index()
decode_review <- function(text, word_index) {
word <- names(word_index)
idx <- unlist(word_index, use.names = FALSE)
word <- c("<PAD>", "<START>", "<UNK>", "<UNUSED>", word)
idx <- c(0:3, idx + 3)
words <- word[match(text, idx, 2)]
paste(words, collapse=" ")
}
decode_review(x_train[[1]][1:12], word_index)
library(Matrix)
one_hot <- function(sequences, dimension) {
seqlen <- sapply(sequences, length) # 提取每个频率长度
n <- length(seqlen)
rowind <- rep(1:n, seqlen) # doc num of each word
colind <- unlist(sequences)
sparseMatrix(i = rowind, j = colind, dims = c(n, dimension))
}
x_train_1h <- one_hot(x_train, 10000)
x_test_1h <- one_hot(x_test, 10000)
dim(x_train_1h)
head(x_train_1h)
nnzero(x_train_1h) / (25000 * 10000)
set.seed(119)
test <- sample(seq(along = y_train), 2000)
library(glmnet)
lm.fit <- glmnet(x_train_1h[-test,], y_train[-test], family = "binomial", standardize = FALSE)
lm.pred <- predict(lm.fit, x_train_1h[test,]) > 0
accuracy <- function(pred, truth) {
mean(drop(pred) == drop(truth))
}
acc.lm <- apply(lm.pred, 2, accuracy, y_train[test] > 0)
lm.test.pred <- predict(lm.fit, x_test_1h) > 0
acc.test.lm <- apply(lm.test.pred, 2, accuracy, y_test > 0)
par(mar = c(4, 4, 4, 4), mfrow = c(1, 1))
plot(-log(lm.fit$lambda), acc.lm)
plot(-log(lm.fit$lambda), acc.test.lm)
model <- keras_model_sequential()
model %>% layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% layer_dense(units = 16, activation = "relu") %>% layer_dense(units = 1, activation = "sigmoid")
model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("accuracy"))
history <- model %>% fit(x_train_1h[-test,], y_train[-test], epochs=20, batch_size=512, validation_data = list(x_train_1h[test, ], y_train[test]))
history <- model %>% fit(x_train_1h[-test,], y_train[-test], epochs=20, batch_size=512, validation_data = list(x_test_1h, y_test))
library(keras)
library(tensorflow)
max_feature <- 1000
imdb <- dataset_imdb(num_words = max_feature)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb
word_index <- dataset_imdb_word_index()
decode_review <- function(text, word_index) {
word <- names(word_index)
idx <- unlist(word_index, use.names = FALSE)
word <- c("<PAD>", "<START>", "<UNK>", "<UNUSED>", word)
idx <- c(0:3, idx + 3)
words <- word[match(text, idx, 2)]
paste(words, collapse = " ")
}
decode_review(x_train[1][1:12], word_index)
library(Matrix)
one_hot <- function(sequences, dimension) {
seqlen <- sapply(sequences, length) # 提取每个频率长度
n <- length(seqlen)
rowind <- rep(1:n, seqlen) # doc num of each word
colind <- unlist(sequences)
sparseMatrix(i = rowind, j = colind, dims = c(n, dimension))
}
x_train_1h <- one_hot(x_train, 10000)
x_test_1h <- one_hot(x_test, 10000)
dim(x_train_1h)
head(x_train_1h)
nnzero(x_train_1h) / (25000 * 10000)
wl <- sapply(x_train, length)
median(wl)
sum(wl <= 500) /length(wl)
maxlen <- 500
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
model <- keras_model_sequential()
model %>% layer_embedding(input_dim = 10000, output_dim = 32) %>%
layer_lstm(units = 32) %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc"))
model %>% fit(x_train, y_train, epochs = 10, batch_size = 128, validation_data = list(x_test, y_test))
pred <- predict(model, x_test) > 0.5
mean(abs(y_test == as.numeric(pred)))
knitr::opts_chunk$set(echo = TRUE)
old_path <- Sys.getenv("PATH")
new_path <- paste(
"C:/Users/dell/miniconda3/envs/r-reticulate/Library/bin",
"C:/Users/dell/miniconda3/envs/r-reticulate/Scripts",
old_path,
sep = ";"
)
library(ISLR2)
library(keras)
library(tensorflow)
library(reticulate)
use_python("C:/Users/dell/miniconda3/envs/r-reticulate/python.exe")
x.data <- data.matrix(
NYSE[, c("DJ_return", "log_volume", "log_volatility")]
)
y <- NYSE[, "train"]
x <- scale(x.data)
# 构造一个用前五天成交数据来预测当天成交量的数据集
lagm <- function(x, k = 1) {
n <- nrow(x)
pad <- matrix(NA, k, ncol(x))
rbind(pad, x[1:(n - k),])
}
arframe <- data.frame(log_volume = x[, "log_volume"], L1 = lagm(x, 1), L2 = lagm(x, 2), L3 = lagm(x, 3), L4 = lagm(x, 4), L5 = lagm(x, 5))
arframe <- arframe[-(1:5),]
y <- y[-(1:5)]
lm.fit <- lm(log_volume~., data = arframe[y,])
lm.pred <- predict(lm.fit, arframe[!y,])
v <- var(arframe[!y, "log_volume"])
1 - mean((lm.pred - arframe[!y, "log_volume"])^2) / v
df <- data.frame(day = NYSE[-(1:5), "day_of_week"], arframe)
lm.fit <- lm(log_volume~., data = df[y,])
lm.pred <- predict(lm.fit, df[!y,])
v <- var(df[!y, "log_volume"])
1 - mean((lm.pred - df[!y, "log_volume"])^2) / v
old_path <- Sys.getenv("PATH")
new_path <- paste(
"C:/Users/dell/miniconda3/envs/r-reticulate/Library/bin",
"C:/Users/dell/miniconda3/envs/r-reticulate/Scripts",
old_path,
sep = ";"
)
library(keras)
library(tensorflow)
library(reticulate)
use_python("C:/Users/dell/miniconda3/envs/r-reticulate/python.exe")
py_config()
max_feature <- 10000
imdb <- dataset_imdb(num_words = max_feature)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb
word_index <- dataset_imdb_word_index()
decode_review <- function(text, word_index) {
word <- names(word_index)
idx <- unlist(word_index, use.names = FALSE)
word <- c("<PAD>", "<START>", "<UNK>", "<UNUSED>", word)
idx <- c(0:3, idx + 3)
words <- word[match(text, idx, 2)]
paste(words, collapse=" ")
}
decode_review(x_train[[1]][1:12], word_index)
library(Matrix)
one_hot <- function(sequences, dimension) {
seqlen <- sapply(sequences, length) # 提取每个频率长度
n <- length(seqlen)
rowind <- rep(1:n, seqlen) # doc num of each word
colind <- unlist(sequences)
sparseMatrix(i = rowind, j = colind, dims = c(n, dimension))
}
x_train_1h <- one_hot(x_train, 10000)
x_test_1h <- one_hot(x_test, 10000)
dim(x_train_1h)
head(x_train_1h)
nnzero(x_train_1h) / (25000 * 10000)
set.seed(119)
test <- sample(seq(along = y_train), 2000)
library(glmnet)
lm.fit <- glmnet(x_train_1h[-test,], y_train[-test], family = "binomial", standardize = FALSE)
lm.pred <- predict(lm.fit, x_train_1h[test,]) > 0
accuracy <- function(pred, truth) {
mean(drop(pred) == drop(truth))
}
acc.lm <- apply(lm.pred, 2, accuracy, y_train[test] > 0)
lm.test.pred <- predict(lm.fit, x_test_1h) > 0
acc.test.lm <- apply(lm.test.pred, 2, accuracy, y_test > 0)
par(mar = c(4, 4, 4, 4), mfrow = c(1, 1))
plot(-log(lm.fit$lambda), acc.lm)
plot(-log(lm.fit$lambda), acc.test.lm)
model <- keras_model_sequential()
model %>% layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% layer_dense(units = 16, activation = "relu") %>% layer_dense(units = 1, activation = "sigmoid")
model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("accuracy"))
history <- model %>% fit(x_train_1h[-test,], y_train[-test], epochs=20, batch_size=512, validation_data = list(x_train_1h[test, ], y_train[test]))
history <- model %>% fit(x_train_1h[-test,], y_train[-test], epochs=20, batch_size=512, validation_data = list(x_test_1h, y_test))
old_path <- Sys.getenv("PATH")
new_path <- paste(
"C:/Users/dell/miniconda3/envs/r-reticulate/Library/bin",
"C:/Users/dell/miniconda3/envs/r-reticulate/Scripts",
old_path,
sep = ";"
)
library(keras)
library(tensorflow)
library(reticulate)
use_python("C:/Users/dell/miniconda3/envs/r-reticulate/python.exe")
py_config()
max_feature <- 1000
imdb <- dataset_imdb(num_words = max_feature)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb
word_index <- dataset_imdb_word_index()
decode_review <- function(text, word_index) {
word <- names(word_index)
idx <- unlist(word_index, use.names = FALSE)
word <- c("<PAD>", "<START>", "<UNK>", "<UNUSED>", word)
idx <- c(0:3, idx + 3)
words <- word[match(text, idx, 2)]
paste(words, collapse = " ")
}
decode_review(x_train[1][1:12], word_index)
library(Matrix)
one_hot <- function(sequences, dimension) {
seqlen <- sapply(sequences, length) # 提取每个频率长度
n <- length(seqlen)
rowind <- rep(1:n, seqlen) # doc num of each word
colind <- unlist(sequences)
sparseMatrix(i = rowind, j = colind, dims = c(n, dimension))
}
x_train_1h <- one_hot(x_train, 10000)
x_test_1h <- one_hot(x_test, 10000)
dim(x_train_1h)
head(x_train_1h)
nnzero(x_train_1h) / (25000 * 10000)
wl <- sapply(x_train, length)
median(wl)
sum(wl <= 500) /length(wl)
maxlen <- 500
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
model <- keras_model_sequential()
model %>% layer_embedding(input_dim = 10000, output_dim = 32) %>%
layer_lstm(units = 32) %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc"))
model %>% fit(x_train, y_train, epochs = 10, batch_size = 128, validation_data = list(x_test, y_test))
pred <- predict(model, x_test) > 0.5
mean(abs(y_test == as.numeric(pred)))
knitr::opts_chunk$set(echo = TRUE)
old_path <- Sys.getenv("PATH")
new_path <- paste(
"C:/Users/dell/miniconda3/envs/r-reticulate/Library/bin",
"C:/Users/dell/miniconda3/envs/r-reticulate/Scripts",
old_path,
sep = ";"
)
library(ISLR2)
library(keras)
library(tensorflow)
library(reticulate)
use_python("C:/Users/dell/miniconda3/envs/r-reticulate/python.exe")
x.data <- data.matrix(
NYSE[, c("DJ_return", "log_volume", "log_volatility")]
)
y <- NYSE[, "train"]
x <- scale(x.data)
# 构造一个用前五天成交数据来预测当天成交量的数据集
lagm <- function(x, k = 1) {
n <- nrow(x)
pad <- matrix(NA, k, ncol(x))
rbind(pad, x[1:(n - k),])
}
arframe <- data.frame(log_volume = x[, "log_volume"], L1 = lagm(x, 1), L2 = lagm(x, 2), L3 = lagm(x, 3), L4 = lagm(x, 4), L5 = lagm(x, 5))
arframe <- arframe[-(1:5),]
y <- y[-(1:5)]
lm.fit <- lm(log_volume~., data = arframe[y,])
lm.pred <- predict(lm.fit, arframe[!y,])
v <- var(arframe[!y, "log_volume"])
1 - mean((lm.pred - arframe[!y, "log_volume"])^2) / v
df <- data.frame(day = NYSE[-(1:5), "day_of_week"], arframe)
lm.fit <- lm(log_volume~., data = df[y,])
lm.pred <- predict(lm.fit, df[!y,])
v <- var(df[!y, "log_volume"])
1 - mean((lm.pred - df[!y, "log_volume"])^2) / v
model <- keras_model_sequential()
model %>% layer_simple_rnn(units = 12, input_shape = list(5, 3), dropout = 0.1, recurrent_dropout = 0.1) %>% layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(), loss = "mse")
model %>% fit(xrnn[y,,], arframe[y, "log_volume"], batch_size = 64, epochs = 200, validation_data = list(xrnn[!y,,], arframe[!y, "log_volume"]))
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 3, 5))
xrnn <- xrnn[,,5:1]
# 转置矩阵，将自变量作为分组依据
xrnn <- aperm(xrnn, c(1, 3, 2))
model <- keras_model_sequential()
model %>% layer_simple_rnn(units = 12, input_shape = list(5, 3), dropout = 0.1, recurrent_dropout = 0.1) %>% layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(), loss = "mse")
model %>% fit(xrnn[y,,], arframe[y, "log_volume"], batch_size = 64, epochs = 200, validation_data = list(xrnn[!y,,], arframe[!y, "log_volume"]))
r.pred <- predict(model, xrnn[!y,,])
1 - mean((r.pred - arframe[!y, "log_volume"]) ^ 2) / v
knitr::opts_chunk$set(echo = TRUE)
old_path <- Sys.getenv("PATH")
new_path <- paste(
"C:/Users/dell/miniconda3/envs/r-reticulate/Library/bin",
"C:/Users/dell/miniconda3/envs/r-reticulate/Scripts",
old_path,
sep = ";"
)
library(keras)
library(tensorflow)
library(reticulate)
use_python("C:/Users/dell/miniconda3/envs/r-reticulate/python.exe")
py_config()
iris <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"), header = FALSE)
head(iris)
dim(iris)
str(iris)
names(iris) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")
iris[, 5] <- as.factor(iris[,5])
plot(iris$Petal.Length, iris$Petal.Width, pch = 21, col = c("red", "green3", "blue")[unclass(iris$Species)], xlab = "Petal Length", ylab = "Petal Width")
cor(iris$Petal.Length, iris$Petal.Width)
m <- cor(iris[, 1:4])
library(corrplot)
corrplot(m)
iris_mat <- as.matrix(iris[, 1:4])
iris_nor <- normalize(iris_mat)
set.seed(19)
ind <- sample(2, nrow(iris), replace = TRUE, prob = c(0.67, 0.33))
iris_train <- iris_nor[ind==1,]
iris_test <- iris_nor[ind==2,]
iris_train_y <- as.numeric(iris[ind==1,5]) - 1
iris_test_y <- as.numeric(iris[ind==2,5]) - 1
iris_train_Labels <- to_categorical(iris_train_y)
iris_test_Labels <- to_categorical(iris_test_y)
print(iris_test_Labels)
model <- keras_model_sequential()
model %>% layer_dense(units = 8, activation = "relu", input_shape = c(4)) %>% layer_dense(units = 3, activation = "softmax")
summary(model)
get_config(model)
get_layer(model, index = 1)
model$layers
model$inputs
model$outputs
model %>% compile(loss = "categorical_crossentropy", optimizer = "SGD", metrics = "accuracy")
model %>% fit(iris_train, iris_train_Labels, epochs = 500, batch_size = 5, validation_split = 0.2)
# 定义要使用的模型
model <- keras_model_sequential()
# 模型定义输入数据(input_shape)为4列,layer_dense为2层，units为每层node数量，activation为每层使用的转换函数。
model %>% layer_dense(units = 8,activation="relu", input_shape = c(4)) %>% layer_dense(units=3, activation= "softmax")
# Print a summary of a model
summary(model)
# Get model configuration
get_config(model)
# Get layer configuration
get_layer(model, index = 1)
# List the model's layers
model$layers
# List the input tensors
model$inputs
# List the output tensors
model$outputs
model %>% compile(loss="categorical_crossentropy", optimizer="SGD", metrics="accuracy")
model %>% fit(iris_train,iris_train_Labels, epochs = 500, batch_size = 5, validation_split = 0.2)
history <- model %>% fit(iris_train, iris_train_Labels, epochs = 500, batch_size = 5, validation_split = 0.2)
plot(history$metrics$loss, main = "Model Loss", xlab = "epoch", ylab = "loss", col="blue", type = "l", ylim = c(0, 1))
lines(history$metrics$val_loss, col = "green")
legend("topright", c("train", "test"), col = c("blue", "green"), lty = c(1, 1))
plot(history$metrics$acc, main = "Model Accuracy", xlab = "epoch", ylab = "accuracy", col = "orange", type = "l", ylim = c(0.5, 1))
lines(history$metrics$val_acc, col = "red")
legend("bottomright", c("train", "test"), col = c("orange", "red"), lty = c(1, 1))
names(iris) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")
iris[,5] <- as.factor(iris[,5])
plot(iris$Petal.Length, iris$Petal.Width, pch=21, col=c("red","green3","blue")[unclass(iris$Species)], xlab="Petal Length", ylab="Petal Width")
cor(iris$Petal.Length, iris$Petal.Width)
m <- cor(iris[,1:4])
library(corrplot)
corrplot(m)
iris_mat <-as.matrix(iris[,1:4])
iris_nor <- normalize(iris_mat)
set.seed(119)
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))
iris_train <- iris_nor[ind==1,]
iris_test <- iris_nor[ind==2,]
iris_train_y <- as.numeric(iris[ind==1,5])-1
iris_test_y <- as.numeric(iris[ind==2,5])-1
# One hot encode training target values
iris_train_Labels <- to_categorical(iris_train_y)
# One hot encode test target values
iris_test_Labels <- to_categorical(iris_test_y)
# Print out the iris.testLabels to double check the result
print(iris_test_Labels)
# 定义要使用的模型
model <- keras_model_sequential()
# 模型定义输入数据(input_shape)为4列,layer_dense为2层，units为每层node数量，activation为每层使用的转换函数。
model %>% layer_dense(units = 8,activation="relu", input_shape = c(4)) %>% layer_dense(units=3, activation= "softmax")
# Print a summary of a model
summary(model)
# Get model configuration
get_config(model)
# Get layer configuration
get_layer(model, index = 1)
# List the model's layers
model$layers
# List the input tensors
model$inputs
# List the output tensors
model$outputs
model %>% compile(loss="categorical_crossentropy", optimizer="SGD", metrics="accuracy")
model %>% fit(iris_train,iris_train_Labels, epochs = 500, batch_size = 5, validation_split = 0.2)
history <- model %>% fit(iris_train, iris_train_Labels, epochs = 500, batch_size = 5, validation_split = 0.2)
plot(history$metrics$loss, main = "Model Loss", xlab = "epoch", ylab = "loss", col="blue", type = "l", ylim = c(0, 1))
lines(history$metrics$val_loss, col = "green")
legend("topright", c("train", "test"), col = c("blue", "green"), lty = c(1, 1))
plot(history$metrics$acc, main = "Model Accuracy", xlab = "epoch", ylab = "accuracy", col = "orange", type = "l", ylim = c(0.5, 1))
lines(history$metrics$val_acc, col = "red")
legend("bottomright", c("train", "test"), col = c("orange", "red"), lty = c(1, 1))
classes <- model %>% predict(iris_test) %>% k_argmax()
score <- model %>% evaluate(iris_test, iris_test_Labels, batch_size = 128)
print(score)
score <- model %>% evaluate(iris_test, iris_test_Labels, batch_size = 128)
print(score)
model1 <- keras_model_sequential()
model1 %>%
layer_dense(units = 8, activation = 'relu', input_shape = c(4)) %>%
layer_dense(units = 5, activation = 'relu') %>%
layer_dense(units = 3, activation = 'softmax')
model1 %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'SGD',
metrics = 'accuracy'
)
model1 %>% fit(iris_train, iris_train_Labels, epochs = 500, batch_size = 5, validation_split = 0.2)
score <- model1 %>% evaluate(iris_test, iris_test_Labels, batch_size = 128)
print(score)
# Initialize the sequential model
model2 <- keras_model_sequential()
# Add layers to model
model2 %>%
layer_dense(units = 24, activation = 'relu', input_shape = c(4)) %>%
layer_dense(units = 3, activation = 'softmax')
model2 %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'SGD',
metrics = 'accuracy'
)
model2 %>% fit(iris_train, iris_train_Labels, epochs = 500, batch_size = 5, validation_split = 0.2)
score <- model2 %>% evaluate(iris_test, iris_test_Labels, batch_size = 128)
print(score)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 8, activation = 'relu', input_shape = c(4)) %>%
layer_dense(units = 3, activation = 'softmax')
model3 %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = 'accuracy'
)
model3 %>% fit(iris_train, iris_train_Labels, epochs = 500, batch_size = 5, validation_set = 0.2)
score <- model3 %>% evaluate(iris_test,iris_test_Labels, batch_size = 128)
print(score)
save_model_hdf5(model, "iris_model.h5")
mode <- load_model_hdf5("iris_model.h5")
json_string <- model_to_json(model)
model <- model_from_json(json_string)
