---
title: "8.3. Bagging and Random Forests"
output: html_notebook
---
 
# 1. Preparation of the data
```{r}
library(tree)
library(MASS)
attach(Boston)
set.seed(2)
dim(Boston)
train = sample(1:nrow(Boston), 300)
boston.train = Boston[train, ]
boston.test = Boston[-train, ]
medv.test = medv[-train]
```
# 2. Fit the Random Forest with Training Set
```{r}
library(randomForest)
rd.boston = randomForest(medv~., data = Boston, subset = train)
rd.boston
summary(rd.boston)
plot(rd.boston)
```

# 3.select the best mtry by the training datasets

```{r}
oob.err = rep(NA, 13)
test.err = rep(NA, 13)
for (mtry in 1:13) {
  fit = randomForest(medv~., data = Boston, subset = train, mtry = mtry, mtree = 400)
  oob.err[mtry] = fit$mse[400]
  pred = predict(fit, boston.test)
  test.err[mtry] = mean((pred - medv.test) ^ 2)
}
matplot(1:mtry, cbind(oob.err, test.err), pch = 19, col = c("red", "blue"), type = "b", ylab = "MSE")
legend("topright", legend = c("OOB", "TEST"), pch = 19, col = c("red", "blue"))
```
# 4.Importance of the Variables

```{r}
rd.boston.best = randomForest(medv~., data = Boston, subset = train, mtry = 5, importance = TRUE)
importance(rd.boston.best)
varImpPlot(rd.boston.best)
```
```{r}
detach(Boston)
```

